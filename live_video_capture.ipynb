{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting new recordings: output_videos\\no_person_1.mp4, output_videos\\person_1.mp4\n",
      "[INFO] Starting new recordings: output_videos\\no_person_2.mp4, output_videos\\person_2.mp4\n",
      "[INFO] Starting new recordings: output_videos\\no_person_2.mp4, output_videos\\person_2.mp4\n",
      "[INFO] Interrupted by user.\n",
      "[INFO] Exiting...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "def live_pose_video_splitter(\n",
    "    output_dir=\"output_videos\",\n",
    "    pose_detection_confidence=0.2,\n",
    "    pose_tracking_confidence=0.2,\n",
    "    buffer_size=60,\n",
    "    max_frames_no_person_after=60,\n",
    "    fourcc_str=\"mp4v\",\n",
    "    fps=30,\n",
    "    consecutive_frames_needed=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Live monitoring of webcam to split into two videos whenever a person is detected:\n",
    "      1) no_person_<ID>.mp4\n",
    "      2) person_<ID>.mp4\n",
    "\n",
    "    Debounced logic:\n",
    "      - Keep a rolling buffer of `buffer_size` frames.\n",
    "      - Require a person to be detected for `consecutive_frames_needed` consecutive frames\n",
    "        before we decide \"a person is in frame\".\n",
    "      - Similarly, require `consecutive_frames_needed` consecutive \"no-person\" frames\n",
    "        before deciding \"the person has left.\"\n",
    "\n",
    "    Process in detail:\n",
    "      1) In the idle state (no videos yet), keep buffering. \n",
    "      2) When we detect a person for N consecutive frames:\n",
    "         - Start new videos (no_person_<ID>.mp4, person_<ID>.mp4).\n",
    "         - Dump the buffer into the no_person video (the \"before\" segment).\n",
    "         - Write ongoing frames to the person video while the person remains in frame.\n",
    "      3) Once the person is absent for N consecutive frames:\n",
    "         - Start counting up to `max_frames_no_person_after`.\n",
    "         - Write those frames to the no_person video.\n",
    "         - Close both videos. Increment video_id.\n",
    "         - Return to idle state.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize Mediapipe Pose\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=pose_detection_confidence,\n",
    "        min_tracking_confidence=pose_tracking_confidence\n",
    "    )\n",
    "\n",
    "    # Initialize webcam capture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    # Get frame dimensions\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Prepare video writer codec\n",
    "    fourcc = cv2.VideoWriter_fourcc(*fourcc_str)\n",
    "\n",
    "    # Rolling buffer for the last `buffer_size` frames\n",
    "    buffer_frames = deque(maxlen=buffer_size)\n",
    "\n",
    "    # State variables\n",
    "    person_in_frame = False           # whether we've declared that a person is in frame\n",
    "    consecutive_person_frames = 0     # how many consecutive frames we've seen a person\n",
    "    consecutive_noperson_frames = 0   # how many consecutive frames we've seen no person\n",
    "\n",
    "    # Video control\n",
    "    video_id = 1\n",
    "    out_no_person = None\n",
    "    out_person = None\n",
    "    recording_person = False\n",
    "    recording_no_person = False\n",
    "    no_person_after_count = 0\n",
    "\n",
    "    def start_new_recordings():\n",
    "        nonlocal out_no_person, out_person, video_id, recording_person, recording_no_person\n",
    "        no_person_path = os.path.join(output_dir, f\"no_person_{video_id}.mp4\")\n",
    "        person_path = os.path.join(output_dir, f\"person_{video_id}.mp4\")\n",
    "\n",
    "        out_no_person = cv2.VideoWriter(no_person_path, fourcc, fps, (width, height))\n",
    "        out_person = cv2.VideoWriter(person_path, fourcc, fps, (width, height))\n",
    "\n",
    "        recording_no_person = True\n",
    "        recording_person = True\n",
    "\n",
    "        print(f\"[INFO] Starting new recordings: {no_person_path}, {person_path}\")\n",
    "\n",
    "    def close_recordings():\n",
    "        nonlocal out_no_person, out_person, recording_person, recording_no_person\n",
    "        if out_no_person is not None:\n",
    "            out_no_person.release()\n",
    "            out_no_person = None\n",
    "        if out_person is not None:\n",
    "            out_person.release()\n",
    "            out_person = None\n",
    "        recording_person = False\n",
    "        recording_no_person = False\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"[INFO] Cannot read from camera or end of stream.\")\n",
    "                break\n",
    "\n",
    "            # Pose detection\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(rgb_frame)\n",
    "            has_person = (results.pose_landmarks is not None)\n",
    "\n",
    "            # Maintain rolling buffer\n",
    "            buffer_frames.append(frame)\n",
    "\n",
    "            # Update consecutive frame counters\n",
    "            if has_person:\n",
    "                consecutive_person_frames += 1\n",
    "                consecutive_noperson_frames = 0\n",
    "            else:\n",
    "                consecutive_noperson_frames += 1\n",
    "                consecutive_person_frames = 0\n",
    "\n",
    "            # --- Handle transitions ---\n",
    "            # 1. If we currently have no person_in_frame,\n",
    "            #    check if we've seen a person for enough consecutive frames to trigger \"enter.\"\n",
    "            if not person_in_frame:\n",
    "                if consecutive_person_frames >= consecutive_frames_needed:\n",
    "                    # A person has \"officially\" entered\n",
    "                    person_in_frame = True\n",
    "                    no_person_after_count = 0  # reset\n",
    "\n",
    "                    # Start new recordings for no_person & person\n",
    "                    start_new_recordings()\n",
    "\n",
    "                    # Dump the buffer into the no_person video\n",
    "                    if out_no_person:\n",
    "                        for bf in buffer_frames:\n",
    "                            out_no_person.write(bf)\n",
    "\n",
    "            else:\n",
    "                # 2. If we currently have person_in_frame == True,\n",
    "                #    check if we've lost the person for enough consecutive frames to trigger \"leave.\"\n",
    "                if consecutive_noperson_frames >= consecutive_frames_needed:\n",
    "                    # Person has \"officially\" left\n",
    "                    person_in_frame = False\n",
    "                    no_person_after_count = 0  # Will start counting after frames for the no_person tail\n",
    "\n",
    "            # --- Recording logic ---\n",
    "            if person_in_frame:\n",
    "                # We are in the \"person present\" phase\n",
    "                if recording_person and out_person:\n",
    "                    out_person.write(frame)\n",
    "            else:\n",
    "                # Person not in frame\n",
    "                if recording_person:\n",
    "                    # The person has recently left, so we need to record the tail of no_person frames\n",
    "                    no_person_after_count += 1\n",
    "                    if recording_no_person and out_no_person:\n",
    "                        out_no_person.write(frame)\n",
    "\n",
    "                    # Once we've written the required tail frames, close the videos\n",
    "                    if no_person_after_count >= max_frames_no_person_after:\n",
    "                        close_recordings()\n",
    "                        video_id += 1\n",
    "\n",
    "                # else: We haven't yet detected a person at all (still in idle),\n",
    "                #       or we finished a cycle and are waiting for the next person.\n",
    "                #       Just keep buffering.\n",
    "\n",
    "            # Show the live feed (optional)\n",
    "            cv2.imshow(\"Live Feed\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"[INFO] Interrupted by user.\")\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        close_recordings()\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"[INFO] Exiting...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    live_pose_video_splitter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera index 0 works!\n",
      "Camera index 1 NOT accessible.\n",
      "Camera index 2 NOT accessible.\n",
      "Camera index 3 NOT accessible.\n",
      "Camera index 4 NOT accessible.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "for idx in range(5):\n",
    "    cap = cv2.VideoCapture(idx)\n",
    "    if cap.isOpened():\n",
    "        print(f\"Camera index {idx} works!\")\n",
    "        cap.release()\n",
    "    else:\n",
    "        print(f\"Camera index {idx} NOT accessible.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
