{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person first detected at frame: 37\n",
      "Person last detected at frame:  126\n",
      "Splitting complete.\n",
      "No-person video saved to: C:\\Users\\joshu\\Documents\\EEE\\AMNIS\\splitter\\no_person.mp4\n",
      "Person video saved to: C:\\Users\\joshu\\Documents\\EEE\\AMNIS\\splitter\\person.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "def split_video_on_pose(\n",
    "    input_video_path ,\n",
    "    output_video_path_no_person,\n",
    "    output_video_path_person,\n",
    "    pose_detection_confidence=0.2,\n",
    "    pose_tracking_confidence=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits an input video into two parts based on pose detection:\n",
    "      1. output_video_path_no_person: concatenated segments where no person is detected\n",
    "      2. output_video_path_person: segment(s) where a person is detected\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize MediaPipe Pose\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=pose_detection_confidence,\n",
    "        min_tracking_confidence=pose_tracking_confidence\n",
    "    )\n",
    "\n",
    "    # Initialize OpenCV video capture\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Prepare for writing videos\n",
    "    # FourCC is a 4-byte code used to specify the video codec.\n",
    "    # On many systems you might use 'mp4v' or 'XVID'.\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    # We will first scan the entire video to find the first and last frame\n",
    "    # where a person is detected (for stable detection).\n",
    "    first_person_frame = None\n",
    "    last_person_frame = None\n",
    "\n",
    "    current_frame_index = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert the BGR image to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Pose detection\n",
    "        results = pose.process(rgb_frame)\n",
    "        \n",
    "        # Check if landmarks are detected\n",
    "        if results.pose_landmarks:\n",
    "            if first_person_frame is None:\n",
    "                first_person_frame = current_frame_index\n",
    "            last_person_frame = current_frame_index\n",
    "        \n",
    "        current_frame_index += 1\n",
    "\n",
    "    # Reset capture to re-read the video from start\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "    if first_person_frame is None or last_person_frame is None:\n",
    "        print(\"No person was detected in the video. Writing the entire video to 'no_person' output.\")\n",
    "        # If no frames had a person, just write the entire video to \"no_person\" and leave \"person\" empty\n",
    "        out_no_person = cv2.VideoWriter(output_video_path_no_person, fourcc, fps, (width, height))\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out_no_person.write(frame)\n",
    "        \n",
    "        out_no_person.release()\n",
    "        cap.release()\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Person first detected at frame: {first_person_frame}\")\n",
    "        print(f\"Person last detected at frame:  {last_person_frame}\")\n",
    "\n",
    "    # Create video writers\n",
    "    out_no_person = cv2.VideoWriter(output_video_path_no_person, fourcc, fps, (width, height))\n",
    "    out_person = cv2.VideoWriter(output_video_path_person, fourcc, fps, (width, height))\n",
    "\n",
    "    # We'll write all frames < first_person_frame to 'no_person'\n",
    "    # We'll write all frames in [first_person_frame, last_person_frame] to 'person'\n",
    "    # We'll write all frames > last_person_frame to 'no_person'\n",
    "    current_frame_index = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if current_frame_index < first_person_frame or current_frame_index > last_person_frame:\n",
    "            out_no_person.write(frame)\n",
    "        else:\n",
    "            out_person.write(frame)\n",
    "        \n",
    "        current_frame_index += 1\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out_no_person.release()\n",
    "    out_person.release()\n",
    "\n",
    "    print(\"Splitting complete.\")\n",
    "    print(f\"No-person video saved to: {output_video_path_no_person}\")\n",
    "    print(f\"Person video saved to: {output_video_path_person}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    input_video = r\"C:\\Users\\joshu\\Queen's University Belfast\\Michael Loughran - raw\\20241122_145206_BR2.mp4\"\n",
    "\n",
    "    # Output paths (ensure these include file names and extensions)\n",
    "    output_no_person = r\"C:\\Users\\joshu\\Documents\\EEE\\AMNIS\\splitter\\no_person.mp4\"\n",
    "    output_person = r\"C:\\Users\\joshu\\Documents\\EEE\\AMNIS\\splitter\\person.mp4\"\n",
    "\n",
    "    # Run the splitting function\n",
    "    split_video_on_pose(\n",
    "        input_video_path=input_video,\n",
    "        output_video_path_no_person=output_no_person,\n",
    "        output_video_path_person=output_person\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
